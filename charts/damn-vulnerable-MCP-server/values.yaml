# Default values for damn-vulnerable-mcp-server
# This is a YAML-formatted file.
# Declare variables to be passed into your templates.

replicaCount: 1

# Docker image configuration
# Default: Uses locally built image (dvmcp:latest)
# Build locally: docker build -t dvmcp:latest https://github.com/harishsg993010/damn-vulnerable-MCP-server.git
# Or use remote repository: set repository to your-registry/dvmcp and pullPolicy to IfNotPresent
image:
  repository: dvmcp
  pullPolicy: Never
  tag: latest

imagePullSecrets: []
nameOverride: ""
fullnameOverride: ""

serviceAccount:
  # Specifies whether a service account should be created
  create: true
  # Automatically mount a ServiceAccount's API credentials?
  automount: true
  # Annotations to add to the service account
  annotations: {}
  # The name of the service account to use.
  # If not set and create is true, a name is generated using the fullname template
  name: ""

podAnnotations: {}
podLabels: {}

podSecurityContext: {}
  # NOTE: Security contexts are disabled by default because this image uses supervisord
  # which requires root privileges. Since this is an educational vulnerability platform
  # meant for local testing only, this is acceptable.
  # For production use, consider:
  # runAsNonRoot: true
  # runAsUser: 1000
  # fsGroup: 2000
  # seccompProfile:
  #   type: RuntimeDefault

securityContext: {}
  # allowPrivilegeEscalation: false
  # capabilities:
  #   drop:
  #   - ALL
  # readOnlyRootFilesystem: false
  # runAsNonRoot: true
  # runAsUser: 1000

service:
  type: NodePort
  annotations: {}
    # service.beta.kubernetes.io/aws-load-balancer-type: nlb
  # Specify nodePort values for each challenge server
  nodePorts:
    challenge1: 30001
    challenge2: 30002
    challenge3: 30003
    challenge4: 30004
    challenge5: 30005
    challenge6: 30006
    challenge7: 30007
    challenge8: 30008
    challenge9: 30009
    challenge10: 30010

  # Service ports for all 10 challenge servers
  ports:
    challenge1:
      port: 9001
      targetPort: 9001
      protocol: TCP
    challenge2:
      port: 9002
      targetPort: 9002
      protocol: TCP
    challenge3:
      port: 9003
      targetPort: 9003
      protocol: TCP
    challenge4:
      port: 9004
      targetPort: 9004
      protocol: TCP
    challenge5:
      port: 9005
      targetPort: 9005
      protocol: TCP
    challenge6:
      port: 9006
      targetPort: 9006
      protocol: TCP
    challenge7:
      port: 9007
      targetPort: 9007
      protocol: TCP
    challenge8:
      port: 9008
      targetPort: 9008
      protocol: TCP
    challenge9:
      port: 9009
      targetPort: 9009
      protocol: TCP
    challenge10:
      port: 9010
      targetPort: 9010
      protocol: TCP

resources:
  # We recommend setting resource limits since 10 servers run in one pod
  limits:
    cpu: 2000m
    memory: 2Gi
  requests:
    cpu: 500m
    memory: 512Mi

# Liveness and readiness probes for health checking
# Note: Disabled by default as the servers may not have a /health endpoint
livenessProbe:
  enabled: false
  httpGet:
    path: /health
    port: 9001
  initialDelaySeconds: 30
  periodSeconds: 10
  timeoutSeconds: 5
  failureThreshold: 3

readinessProbe:
  enabled: false
  httpGet:
    path: /health
    port: 9001
  initialDelaySeconds: 15
  periodSeconds: 5
  timeoutSeconds: 3
  failureThreshold: 3

# Autoscaling configuration (not recommended for this use case)
autoscaling:
  enabled: false
  minReplicas: 1
  maxReplicas: 3
  targetCPUUtilizationPercentage: 80
  # targetMemoryUtilizationPercentage: 80

# Additional volumes on the output Deployment definition.
volumes: []
# - name: foo
#   secret:
#     secretName: mysecret
#     optional: false

# Additional volumeMounts on the output Deployment definition.
volumeMounts: []
# - name: foo
#   mountPath: "/etc/foo"
#   readOnly: true

nodeSelector: {}

tolerations: []

affinity: {}

# Persistence configuration for challenge data
persistence:
  enabled: false
  annotations: {}
  storageClass: ""
  # Use "-" to disable dynamic provisioning
  accessMode: ReadWriteOnce
  size: 1Gi
  existingClaim: ""
  # Challenge data directories that will be persisted
  # These match the /tmp/dvmcp_challenge* directories created in the Dockerfile
  mountPaths:
    challenge3: /tmp/dvmcp_challenge3
    challenge4: /tmp/dvmcp_challenge4
    challenge6: /tmp/dvmcp_challenge6
    challenge8: /tmp/dvmcp_challenge8
    challenge10: /tmp/dvmcp_challenge10
